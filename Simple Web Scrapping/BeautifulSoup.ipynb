from selenium import webdriver
from selenium.webdriver.chrome.service import Service as ChromeService
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import time

# Setup
options = Options()
options.add_argument("--headless")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)

base_url = "" # use a website of your choice
max_page = 5  # As discovered from the HTML

all_csos = []

for page in range(1, max_page + 1):
    if page == 1:
        url = base_url
    else:
        url = f"{base_url}{page}/"
    
    print(f"Scraping page {page} -> {url}")
    driver.get(url)
    time.sleep(2)  # wait for JS to render
    
    soup = BeautifulSoup(driver.page_source, "html.parser")
    
    for person in soup.select("div.e-loop-item"):
        name_tag = person.select_one("h4.elementor-heading-title")
        title_tags = person.select("span.elementor-heading-title")

        name = name_tag.get_text(strip=True) if name_tag else ""
        title = title_tags[0].get_text(strip=True) if len(title_tags) > 0 else ""
        company = title_tags[2].get_text(strip=True) if len(title_tags) > 2 else ""

        links = person.select("a.elementor-icon")
        linkedin = ""
        report = ""

        for link in links:
            href = link.get("href", "")
            if "linkedin.com" in href:
                linkedin = href
            elif "http" in href:
                report = href

        all_csos.append({
            "name": name,
            "title": title,
            "company": company,
            "linkedin": linkedin,
            "report": report
        })

driver.quit()

# Show results
for cso in all_csos:
    print(cso)

print(f"\n Total CSOs scraped: {len(all_csos)}")
